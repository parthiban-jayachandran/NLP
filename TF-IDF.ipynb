{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequence - Inverse Document Frequency(TF-IDF) Using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "text = [\"Try Checking the network cables, Try Checking the modem, and Try Checking the router. Reconnecting to WiFi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'try': 9, 'checking': 2, 'the': 7, 'network': 4, 'cables': 1, 'modem': 3, 'and': 0, 'router': 6, 'reconnecting': 5, 'to': 8, 'wifi': 10}\n"
     ]
    }
   ],
   "source": [
    "vector = CountVectorizer()\n",
    "vector1 = vector.fit(text)\n",
    "vector2 = vector.transform(text)\n",
    "print(vector1.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 3 1 1 1 1 3 1 3 1]]\n"
     ]
    }
   ],
   "source": [
    "print(vector2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vector_tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [\"Try Checking the network cables\", \"Try Checking the modem\", \"and Try Checking the router\", \"Reconnecting to WiFi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'try': 9, 'checking': 2, 'the': 7, 'network': 4, 'cables': 1, 'modem': 3, 'and': 0, 'router': 6, 'reconnecting': 5, 'to': 8, 'wifi': 10}\n"
     ]
    }
   ],
   "source": [
    "vector3 = vector_tfidf.fit(test)\n",
    "print(vector3.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.55708525 0.3555803  0.         0.55708525 0.\n",
      "  0.         0.3555803  0.         0.3555803  0.        ]\n",
      " [0.         0.         0.42817512 0.67081906 0.         0.\n",
      "  0.         0.42817512 0.         0.42817512 0.        ]\n",
      " [0.55708525 0.         0.3555803  0.         0.         0.\n",
      "  0.55708525 0.3555803  0.         0.3555803  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.57735027\n",
      "  0.         0.         0.57735027 0.         0.57735027]]\n"
     ]
    }
   ],
   "source": [
    "vector4 = vector_tfidf.transform(test)\n",
    "print(vector4.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.55708525 0.3555803  0.         0.55708525 0.\n",
      "  0.         0.3555803  0.         0.3555803  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "test1 = test[0]\n",
    "test1_tfidf = vector_tfidf.transform([test1])\n",
    "print(test1_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'try': 4, 'checking': 1, 'the': 3, 'network': 2, 'cables': 0}\n"
     ]
    }
   ],
   "source": [
    "test1_tfidf = vector_tfidf.fit([test1])\n",
    "print(test1_tfidf.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Using gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pprint\n",
    "from gensim import corpora\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list = [\"This is a book about Natural Language Processing.\",\n",
    "'By \"natural language\" we mean a language that is used for everyday communication by humans; languages like English, Hindi or Portuguese.',\n",
    "\"In contrast to artificial languages such as programming languages and mathematical notations,natural languages have evolved as they pass from generation to generation, and are hard to pin down with explicit rules. \",\n",
    "\"We will take Natural Language Processing — or NLP for short — in a wide sense to cover any kind of computer manipulation of natural language. \",\n",
    "\"At one extreme, it could be as simple as counting word frequencies to compare different writing styles. \",\n",
    "\"At the other extreme, NLP involves understanding complete human utterances,at least to the extent of being able to give useful responses to them.\"]\n",
    "\n",
    "doc_list2 = [\"Hello, how are you ?\", \"How do you do ?\",\n",
    "           \"Hey what are you doing ?\", \"yes you what are you doing ?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_token = [simple_preprocess(doc) for doc in doc_list]\n",
    "dictionary = corpora.Dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)],\n",
       " [(2, 1),\n",
       "  (3, 2),\n",
       "  (4, 1),\n",
       "  (7, 2),\n",
       "  (8, 1),\n",
       "  (9, 1),\n",
       "  (10, 1),\n",
       "  (11, 1),\n",
       "  (12, 1),\n",
       "  (13, 1),\n",
       "  (14, 1),\n",
       "  (15, 1),\n",
       "  (16, 1),\n",
       "  (17, 1),\n",
       "  (18, 1),\n",
       "  (19, 1),\n",
       "  (20, 1),\n",
       "  (21, 1)],\n",
       " [(4, 1),\n",
       "  (14, 3),\n",
       "  (22, 2),\n",
       "  (23, 1),\n",
       "  (24, 1),\n",
       "  (25, 2),\n",
       "  (26, 1),\n",
       "  (27, 1),\n",
       "  (28, 1),\n",
       "  (29, 1),\n",
       "  (30, 1),\n",
       "  (31, 2),\n",
       "  (32, 1),\n",
       "  (33, 1),\n",
       "  (34, 1),\n",
       "  (35, 1),\n",
       "  (36, 1),\n",
       "  (37, 1),\n",
       "  (38, 1),\n",
       "  (39, 1),\n",
       "  (40, 1),\n",
       "  (41, 1),\n",
       "  (42, 1),\n",
       "  (43, 3),\n",
       "  (44, 1)],\n",
       " [(3, 2),\n",
       "  (4, 2),\n",
       "  (5, 1),\n",
       "  (11, 1),\n",
       "  (17, 1),\n",
       "  (21, 1),\n",
       "  (34, 1),\n",
       "  (43, 1),\n",
       "  (45, 1),\n",
       "  (46, 1),\n",
       "  (47, 1),\n",
       "  (48, 1),\n",
       "  (49, 1),\n",
       "  (50, 1),\n",
       "  (51, 2),\n",
       "  (52, 1),\n",
       "  (53, 1),\n",
       "  (54, 1),\n",
       "  (55, 1),\n",
       "  (56, 1)],\n",
       " [(25, 2),\n",
       "  (43, 1),\n",
       "  (57, 1),\n",
       "  (58, 1),\n",
       "  (59, 1),\n",
       "  (60, 1),\n",
       "  (61, 1),\n",
       "  (62, 1),\n",
       "  (63, 1),\n",
       "  (64, 1),\n",
       "  (65, 1),\n",
       "  (66, 1),\n",
       "  (67, 1),\n",
       "  (68, 1),\n",
       "  (69, 1),\n",
       "  (70, 1)],\n",
       " [(43, 3),\n",
       "  (50, 1),\n",
       "  (51, 1),\n",
       "  (57, 2),\n",
       "  (63, 1),\n",
       "  (71, 1),\n",
       "  (72, 1),\n",
       "  (73, 1),\n",
       "  (74, 1),\n",
       "  (75, 1),\n",
       "  (76, 1),\n",
       "  (77, 1),\n",
       "  (78, 1),\n",
       "  (79, 1),\n",
       "  (80, 1),\n",
       "  (81, 2),\n",
       "  (82, 1),\n",
       "  (83, 1),\n",
       "  (84, 1),\n",
       "  (85, 1)]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc, allow_update=True) for doc in doc_token]\n",
    "bow_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['about', 1], ['book', 1], ['is', 1], ['language', 1], ['natural', 1], ['processing', 1], ['this', 1]]\n",
      "[['is', 1], ['language', 2], ['natural', 1], ['by', 2], ['communication', 1], ['english', 1], ['everyday', 1], ['for', 1], ['hindi', 1], ['humans', 1], ['languages', 1], ['like', 1], ['mean', 1], ['or', 1], ['portuguese', 1], ['that', 1], ['used', 1], ['we', 1]]\n",
      "[['natural', 1], ['languages', 3], ['and', 2], ['are', 1], ['artificial', 1], ['as', 2], ['contrast', 1], ['down', 1], ['evolved', 1], ['explicit', 1], ['from', 1], ['generation', 2], ['hard', 1], ['have', 1], ['in', 1], ['mathematical', 1], ['notations', 1], ['pass', 1], ['pin', 1], ['programming', 1], ['rules', 1], ['such', 1], ['they', 1], ['to', 3], ['with', 1]]\n",
      "[['language', 2], ['natural', 2], ['processing', 1], ['for', 1], ['or', 1], ['we', 1], ['in', 1], ['to', 1], ['any', 1], ['computer', 1], ['cover', 1], ['kind', 1], ['manipulation', 1], ['nlp', 1], ['of', 2], ['sense', 1], ['short', 1], ['take', 1], ['wide', 1], ['will', 1]]\n",
      "[['as', 2], ['to', 1], ['at', 1], ['be', 1], ['compare', 1], ['could', 1], ['counting', 1], ['different', 1], ['extreme', 1], ['frequencies', 1], ['it', 1], ['one', 1], ['simple', 1], ['styles', 1], ['word', 1], ['writing', 1]]\n",
      "[['to', 3], ['nlp', 1], ['of', 1], ['at', 2], ['extreme', 1], ['able', 1], ['being', 1], ['complete', 1], ['extent', 1], ['give', 1], ['human', 1], ['involves', 1], ['least', 1], ['other', 1], ['responses', 1], ['the', 2], ['them', 1], ['understanding', 1], ['useful', 1], ['utterances', 1]]\n"
     ]
    }
   ],
   "source": [
    "for doc in bow_corpus:\n",
    "    print([[dictionary[id],freq] for id,freq in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(43, 3),\n",
       " (50, 1),\n",
       " (51, 1),\n",
       " (57, 2),\n",
       " (63, 1),\n",
       " (71, 1),\n",
       " (72, 1),\n",
       " (73, 1),\n",
       " (74, 1),\n",
       " (75, 1),\n",
       " (76, 1),\n",
       " (77, 1),\n",
       " (78, 1),\n",
       " (79, 1),\n",
       " (80, 1),\n",
       " (81, 2),\n",
       " (82, 1),\n",
       " (83, 1),\n",
       " (84, 1),\n",
       " (85, 1)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['about', 0.49], ['book', 0.49], ['is', 0.32], ['language', 0.22], ['natural', 0.14], ['processing', 0.32], ['this', 0.49]]\n",
      "[['is', 0.16], ['language', 0.21], ['natural', 0.07], ['by', 0.49], ['communication', 0.24], ['english', 0.24], ['everyday', 0.24], ['for', 0.16], ['hindi', 0.24], ['humans', 0.24], ['languages', 0.16], ['like', 0.24], ['mean', 0.24], ['or', 0.16], ['portuguese', 0.24], ['that', 0.24], ['used', 0.24], ['we', 0.16]]\n",
      "[['natural', 0.05], ['languages', 0.34], ['and', 0.35], ['are', 0.18], ['artificial', 0.18], ['as', 0.23], ['contrast', 0.18], ['down', 0.18], ['evolved', 0.18], ['explicit', 0.18], ['from', 0.18], ['generation', 0.35], ['hard', 0.18], ['have', 0.18], ['in', 0.11], ['mathematical', 0.18], ['notations', 0.18], ['pass', 0.18], ['pin', 0.18], ['programming', 0.18], ['rules', 0.18], ['such', 0.18], ['they', 0.18], ['to', 0.15], ['with', 0.18]]\n",
      "[['language', 0.22], ['natural', 0.15], ['processing', 0.16], ['for', 0.16], ['or', 0.16], ['we', 0.16], ['in', 0.16], ['to', 0.07], ['any', 0.26], ['computer', 0.26], ['cover', 0.26], ['kind', 0.26], ['manipulation', 0.26], ['nlp', 0.16], ['of', 0.33], ['sense', 0.26], ['short', 0.26], ['take', 0.26], ['wide', 0.26], ['will', 0.26]]\n",
      "[['as', 0.34], ['to', 0.08], ['at', 0.17], ['be', 0.26], ['compare', 0.26], ['could', 0.26], ['counting', 0.26], ['different', 0.26], ['extreme', 0.17], ['frequencies', 0.26], ['it', 0.26], ['one', 0.26], ['simple', 0.26], ['styles', 0.26], ['word', 0.26], ['writing', 0.26]]\n",
      "[['to', 0.19], ['nlp', 0.14], ['of', 0.14], ['at', 0.28], ['extreme', 0.14], ['able', 0.21], ['being', 0.21], ['complete', 0.21], ['extent', 0.21], ['give', 0.21], ['human', 0.21], ['involves', 0.21], ['least', 0.21], ['other', 0.21], ['responses', 0.21], ['the', 0.43], ['them', 0.21], ['understanding', 0.21], ['useful', 0.21], ['utterances', 0.21]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gensim import models\n",
    "from gensim.models import TfidfModel\n",
    "tfidf = gensim.models.TfidfModel(bow_corpus, smartirs='ntc')\n",
    "for doc in tfidf[bow_corpus]:\n",
    "    print([[dictionary[id], np.around(freq, decimals=2)] for id,freq in doc])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
