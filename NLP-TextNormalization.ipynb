{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Normalization : SpaCy vs NLTK "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def spacy_fun(text):\n",
    "    spacy_form = nlp(text)\n",
    "    \n",
    "    print('\\nSpaCy Text Normalization:\\n')\n",
    "    \n",
    "    #Tokenization and Lemmatization in spacy\n",
    "    lem_word = []\n",
    "    for i in spacy_form:\n",
    "        lem_word.append(i.lemma_)\n",
    "        \n",
    "    print('Tokenization and Lemmatization in Spacy:\\n')\n",
    "    print(lem_word)\n",
    "    \n",
    "    #Removing stop words in the text\n",
    "    stop_word = []\n",
    "    for word in lem_word:\n",
    "        vocab_word = nlp.vocab[word]\n",
    "        if vocab_word.is_stop == False:\n",
    "            stop_word.append(word)\n",
    "    print('\\nStop word regularization: \\n')\n",
    "    print(stop_word)\n",
    "    \n",
    "    #Removing punctuations in the text\n",
    "    punctuation = '#?!-,.;:–/—'\n",
    "    for word in stop_word:\n",
    "        if word in punctuation:\n",
    "            stop_word.remove(word)\n",
    "    print(\"\\nAfter removing punctuations\\n\")\n",
    "    print(stop_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "ps = PorterStemmer()\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def nlkt_fun(text):\n",
    "    \n",
    "    print('\\nNLTK Text Normalization:\\n')\n",
    "    \n",
    "    #Tokenizing the raw text\n",
    "    word_token = word_tokenize(text)\n",
    "    print('Tokenzing the raw text:\\n')\n",
    "    print(word_token)\n",
    "    \n",
    "    #Stemming the data\n",
    "    word_stem = []\n",
    "    for word in word_token:\n",
    "        word_stem.append(ps.stem(word))\n",
    "    print('\\nStemming the data:\\n')\n",
    "    print(word_stem)\n",
    "    \n",
    "    #Lemmatizing the data\n",
    "    word_lemma =[]\n",
    "    for word2 in word_stem:\n",
    "        word_lemma.append(lemma.lemmatize(word2))\n",
    "    print('\\nLemmatizing the data:\\n')\n",
    "    print(word_lemma)\n",
    "    \n",
    "    #Removing stop words from the list\n",
    "    word_stop = []\n",
    "    nltk_stop_words = set(stopwords.words('english'))\n",
    "    for word3 in word_lemma:\n",
    "        if word3 not in nltk_stop_words:\n",
    "            word_stop.append(word3)\n",
    "            \n",
    "    print('\\nRemoving stop words from the list:\\n')\n",
    "    print(word_stop)\n",
    "    \n",
    "    #Removing the punctuations from the data\n",
    "    punctuation = '#?!-,.;:–/—'\n",
    "    for word4 in word_stop:\n",
    "        if word4 in punctuation:\n",
    "            word_stop.remove(word4)\n",
    "    print('\\nRemoving the punctuations from the data:\\n')\n",
    "    print(word_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet1 = \"#Millions of small businesses – especially those owned by women and people of color – are struggling to keep their doors open. Today, our Administration announced key changes to the Paycheck Protection Program that will help get relief to more small businesses across the country.\"\n",
    "tweet2 = \"Texas — If you’re without heat, head to http://tdem.texas.gov/warm to find a warming shelter near you or call 211 for additional assistance.\"\n",
    "tweet3 = 'Our 7-day daily average of 1.7 million doses administered compares to an average of 892k the week before President Biden took office. That’s almost double in just four weeks. Kamala harris'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SpaCy Text Normalization:\n",
      "\n",
      "Tokenization and Lemmatization in Spacy:\n",
      "\n",
      "['#', 'million', 'of', 'small', 'business', '–', 'especially', 'those', 'own', 'by', 'woman', 'and', 'people', 'of', 'color', '–', 'be', 'struggle', 'to', 'keep', 'their', 'door', 'open', '.', 'today', ',', 'our', 'Administration', 'announce', 'key', 'change', 'to', 'the', 'Paycheck', 'Protection', 'Program', 'that', 'will', 'help', 'get', 'relief', 'to', 'more', 'small', 'business', 'across', 'the', 'country', '.']\n",
      "\n",
      "Stop word regularization: \n",
      "\n",
      "['#', 'million', 'small', 'business', '–', 'especially', 'woman', 'people', 'color', '–', 'struggle', 'door', 'open', '.', 'today', ',', 'Administration', 'announce', 'key', 'change', 'Paycheck', 'Protection', 'Program', 'help', 'relief', 'small', 'business', 'country', '.']\n",
      "\n",
      "After removing punctuations\n",
      "\n",
      "['million', 'small', 'business', 'especially', 'woman', 'people', 'color', 'struggle', 'door', 'open', 'today', 'Administration', 'announce', 'key', 'change', 'Paycheck', 'Protection', 'Program', 'help', 'relief', 'small', 'business', 'country']\n",
      "Wall time: 37.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "spacy_fun(tweet1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NLTK Text Normalization:\n",
      "\n",
      "Tokenzing the raw text:\n",
      "\n",
      "['#', 'Millions', 'of', 'small', 'businesses', '–', 'especially', 'those', 'owned', 'by', 'women', 'and', 'people', 'of', 'color', '–', 'are', 'struggling', 'to', 'keep', 'their', 'doors', 'open', '.', 'Today', ',', 'our', 'Administration', 'announced', 'key', 'changes', 'to', 'the', 'Paycheck', 'Protection', 'Program', 'that', 'will', 'help', 'get', 'relief', 'to', 'more', 'small', 'businesses', 'across', 'the', 'country', '.']\n",
      "\n",
      "Stemming the data:\n",
      "\n",
      "['#', 'million', 'of', 'small', 'busi', '–', 'especi', 'those', 'own', 'by', 'women', 'and', 'peopl', 'of', 'color', '–', 'are', 'struggl', 'to', 'keep', 'their', 'door', 'open', '.', 'today', ',', 'our', 'administr', 'announc', 'key', 'chang', 'to', 'the', 'paycheck', 'protect', 'program', 'that', 'will', 'help', 'get', 'relief', 'to', 'more', 'small', 'busi', 'across', 'the', 'countri', '.']\n",
      "\n",
      "Lemmatizing the data:\n",
      "\n",
      "['#', 'million', 'of', 'small', 'busi', '–', 'especi', 'those', 'own', 'by', 'woman', 'and', 'peopl', 'of', 'color', '–', 'are', 'struggl', 'to', 'keep', 'their', 'door', 'open', '.', 'today', ',', 'our', 'administr', 'announc', 'key', 'chang', 'to', 'the', 'paycheck', 'protect', 'program', 'that', 'will', 'help', 'get', 'relief', 'to', 'more', 'small', 'busi', 'across', 'the', 'countri', '.']\n",
      "\n",
      "Removing stop words from the list:\n",
      "\n",
      "['#', 'million', 'small', 'busi', '–', 'especi', 'woman', 'peopl', 'color', '–', 'struggl', 'keep', 'door', 'open', '.', 'today', ',', 'administr', 'announc', 'key', 'chang', 'paycheck', 'protect', 'program', 'help', 'get', 'relief', 'small', 'busi', 'across', 'countri', '.']\n",
      "\n",
      "Removing the punctuations from the data:\n",
      "\n",
      "['million', 'small', 'busi', 'especi', 'woman', 'peopl', 'color', 'struggl', 'keep', 'door', 'open', 'today', 'administr', 'announc', 'key', 'chang', 'paycheck', 'protect', 'program', 'help', 'get', 'relief', 'small', 'busi', 'across', 'countri']\n",
      "Wall time: 2.82 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nlkt_fun(tweet1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SpaCy Text Normalization:\n",
      "\n",
      "Tokenization and Lemmatization in Spacy:\n",
      "\n",
      "['Texas', '—', 'if', 'you', '’re', 'without', 'heat', ',', 'head', 'to', 'http://tdem.texas.gov/warm', 'to', 'find', 'a', 'warming', 'shelter', 'near', 'you', 'or', 'call', '211', 'for', 'additional', 'assistance', '.']\n",
      "\n",
      "Stop word regularization: \n",
      "\n",
      "['Texas', '—', 'heat', ',', 'head', 'http://tdem.texas.gov/warm', 'find', 'warming', 'shelter', 'near', '211', 'additional', 'assistance', '.']\n",
      "\n",
      "After removing punctuations\n",
      "\n",
      "['Texas', 'heat', 'head', 'http://tdem.texas.gov/warm', 'find', 'warming', 'shelter', 'near', '211', 'additional', 'assistance']\n",
      "Wall time: 19.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "spacy_fun(tweet2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NLTK Text Normalization:\n",
      "\n",
      "Tokenzing the raw text:\n",
      "\n",
      "['Texas', '—', 'If', 'you', '’', 're', 'without', 'heat', ',', 'head', 'to', 'http', ':', '//tdem.texas.gov/warm', 'to', 'find', 'a', 'warming', 'shelter', 'near', 'you', 'or', 'call', '211', 'for', 'additional', 'assistance', '.']\n",
      "\n",
      "Stemming the data:\n",
      "\n",
      "['texa', '—', 'If', 'you', '’', 're', 'without', 'heat', ',', 'head', 'to', 'http', ':', '//tdem.texas.gov/warm', 'to', 'find', 'a', 'warm', 'shelter', 'near', 'you', 'or', 'call', '211', 'for', 'addit', 'assist', '.']\n",
      "\n",
      "Lemmatizing the data:\n",
      "\n",
      "['texa', '—', 'If', 'you', '’', 're', 'without', 'heat', ',', 'head', 'to', 'http', ':', '//tdem.texas.gov/warm', 'to', 'find', 'a', 'warm', 'shelter', 'near', 'you', 'or', 'call', '211', 'for', 'addit', 'assist', '.']\n",
      "\n",
      "Removing stop words from the list:\n",
      "\n",
      "['texa', '—', 'If', '’', 'without', 'heat', ',', 'head', 'http', ':', '//tdem.texas.gov/warm', 'find', 'warm', 'shelter', 'near', 'call', '211', 'addit', 'assist', '.']\n",
      "\n",
      "Removing the punctuations from the data:\n",
      "\n",
      "['texa', 'If', '’', 'without', 'heat', 'head', 'http', '//tdem.texas.gov/warm', 'find', 'warm', 'shelter', 'near', 'call', '211', 'addit', 'assist']\n",
      "Wall time: 7.98 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nlkt_fun(tweet2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SpaCy Text Normalization:\n",
      "\n",
      "Tokenization and Lemmatization in Spacy:\n",
      "\n",
      "['our', '7', '-', 'day', 'daily', 'average', 'of', '1.7', 'million', 'dose', 'administer', 'compare', 'to', 'an', 'average', 'of', '892k', 'the', 'week', 'before', 'President', 'Biden', 'take', 'office', '.', 'that', '’', 'almost', 'double', 'in', 'just', 'four', 'week', '.', 'Kamala', 'harris']\n",
      "\n",
      "Stop word regularization: \n",
      "\n",
      "['7', '-', 'day', 'daily', 'average', '1.7', 'million', 'dose', 'administer', 'compare', 'average', '892k', 'week', 'President', 'Biden', 'office', '.', '’', 'double', 'week', '.', 'Kamala', 'harris']\n",
      "\n",
      "After removing punctuations\n",
      "\n",
      "['7', 'day', 'daily', 'average', '1.7', 'million', 'dose', 'administer', 'compare', 'average', '892k', 'week', 'President', 'Biden', 'office', '’', 'double', 'week', 'Kamala', 'harris']\n",
      "Wall time: 24.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "spacy_fun(tweet3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NLTK Text Normalization:\n",
      "\n",
      "Tokenzing the raw text:\n",
      "\n",
      "['Our', '7-day', 'daily', 'average', 'of', '1.7', 'million', 'doses', 'administered', 'compares', 'to', 'an', 'average', 'of', '892k', 'the', 'week', 'before', 'President', 'Biden', 'took', 'office', '.', 'That', '’', 's', 'almost', 'double', 'in', 'just', 'four', 'weeks', '.', 'Kamala', 'harris']\n",
      "\n",
      "Stemming the data:\n",
      "\n",
      "['our', '7-day', 'daili', 'averag', 'of', '1.7', 'million', 'dose', 'administ', 'compar', 'to', 'an', 'averag', 'of', '892k', 'the', 'week', 'befor', 'presid', 'biden', 'took', 'offic', '.', 'that', '’', 's', 'almost', 'doubl', 'in', 'just', 'four', 'week', '.', 'kamala', 'harri']\n",
      "\n",
      "Lemmatizing the data:\n",
      "\n",
      "['our', '7-day', 'daili', 'averag', 'of', '1.7', 'million', 'dose', 'administ', 'compar', 'to', 'an', 'averag', 'of', '892k', 'the', 'week', 'befor', 'presid', 'biden', 'took', 'offic', '.', 'that', '’', 's', 'almost', 'doubl', 'in', 'just', 'four', 'week', '.', 'kamala', 'harri']\n",
      "\n",
      "Removing stop words from the list:\n",
      "\n",
      "['7-day', 'daili', 'averag', '1.7', 'million', 'dose', 'administ', 'compar', 'averag', '892k', 'week', 'befor', 'presid', 'biden', 'took', 'offic', '.', '’', 'almost', 'doubl', 'four', 'week', '.', 'kamala', 'harri']\n",
      "\n",
      "Removing the punctuations from the data:\n",
      "\n",
      "['7-day', 'daili', 'averag', '1.7', 'million', 'dose', 'administ', 'compar', 'averag', '892k', 'week', 'befor', 'presid', 'biden', 'took', 'offic', '’', 'almost', 'doubl', 'four', 'week', 'kamala', 'harri']\n",
      "Wall time: 7.98 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nlkt_fun(tweet3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today NOUN npadvmod today\n",
      ", PUNCT punct ,\n",
      "America PROPN nsubj America\n",
      "is AUX ROOT be\n",
      "officially ADV advmod officially\n",
      "back ADV advmod back\n",
      "in ADP prep in\n",
      "the DET det the\n",
      "Paris PROPN compound Paris\n",
      "Climate PROPN compound Climate\n",
      "Agreement PROPN pobj Agreement\n",
      ". PUNCT punct .\n",
      "Let VERB ROOT let\n",
      "’s PRON nsubj ’s\n",
      "get VERB ccomp get\n",
      "to ADP prep to\n",
      "work NOUN pobj work\n",
      ".. PUNCT punct ..\n",
      "Wall time: 748 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Import spaCy and load the language library\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "Example_Sentence = \"Today, America is officially back in the Paris Climate Agreement. Let’s get to work..\"\n",
    "\n",
    "# Create a Doc object\n",
    "doc = nlp(Example_Sentence)\n",
    "\n",
    "# Print each token separately\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'about', 'together', 'part', 'please', 'often', 'there', '’d', 'him', 'may', 'that', 'using', 'though', 'thereby', 'thence', 'bottom', 'for', 'back', 'hereafter', 'at', 'is', 'take', 'than', 'yours', \"'m\", 'much', 'as', 'not', 'whereas', 'will', 'therefore', 'hereupon', 'serious', 'me', 'former', 'hereby', 'meanwhile', 'beforehand', 'its', 'nobody', 'they', 'move', 'an', 'been', 'myself', 'herself', 'either', 'after', 'anything', 'name', 'whither', 'i', 'well', 'too', 'cannot', 'since', 'else', '’s', 'twelve', 'while', 'third', 'least', 'also', 'moreover', 'those', 'whereupon', 'nowhere', 'could', 'nothing', 'almost', 'across', 'although', 'sometime', 'per', '‘re', 'nine', 'regarding', 'ourselves', 'thus', 'many', 'less', 'first', 'ever', 'of', 'behind', 'in', 'whereby', 'do', 'on', 'most', 'before', 'wherein', 'anywhere', 'wherever', 'this', 'a', 'once', \"'s\", 'amount', 'seem', '’re', 'can', 'made', 'these', 'hers', '‘ll', 'any', 'being', 'doing', 'again', 'anyone', 'own', 'perhaps', 'seems', 'he', 'should', 'due', 'below', 'you', 'front', 'sometimes', '’ve', 'between', 'everything', 'were', 'only', 'here', 'mine', 'both', 'thru', 'towards', 'his', 'another', 'whose', 'them', 'above', 'somewhere', 'used', 'or', 'noone', 'until', 'rather', \"'d\", 'otherwise', 'side', 'now', 'hundred', 'beside', 'why', 'how', 'whereafter', 'see', 'have', 'latterly', 'if', 'herein', \"'ve\", 'through', 'everyone', 'among', 'yourself', 'whoever', 'every', 'must', 'further', 'eight', \"'ll\", 'fifty', 'three', 'besides', 'am', 'latter', 'something', '‘d', 'itself', 'anyhow', 'without', 'everywhere', 'four', 'therein', 'already', 'where', 'during', 'toward', 'thereafter', 'yet', 'really', \"'re\", 'indeed', 'except', '‘s', 'get', 'off', 'all', 'call', 'under', 'from', 'with', 'somehow', 'sixty', 'give', 'us', '’ll', 'our', 'make', 'around', 'beyond', 'several', 'your', 'n’t', 'whatever', 'when', 'two', 'elsewhere', 'into', 'whenever', 'by', 'one', 'to', 'last', 'within', 'more', 'which', 'other', 'their', 'against', 'same', 'was', 'ten', 'done', 'became', 'none', 'next', 'seeming', 'put', 'various', 'because', 'enough', 'quite', '‘ve', 'ca', 'alone', 'whole', 'she', 'and', 'we', 'fifteen', 're', 'ours', 'has', \"n't\", 'namely', 'then', 'n‘t', 'yourselves', 'be', 'becoming', 'always', 'out', 'mostly', 'never', 'himself', 'upon', 'each', 'seemed', 'even', 'however', 'might', 'six', 'no', 'along', 'the', 'are', 'it', 'had', 'my', 'onto', 'few', 'anyway', 'thereupon', 'forty', 'five', 'become', 'hence', 'whether', 'so', 'eleven', 'over', 'themselves', 'afterwards', 'empty', 'say', 'amongst', 'others', 'does', 'who', 'someone', 'down', 'nevertheless', 'just', 'still', 'what', 'top', 'formerly', '’m', 'some', 'very', 'did', 'full', 'via', 'throughout', '‘m', 'up', 'whom', 'keep', 'her', 'go', 'twenty', 'unless', 'such', 'show', 'would', 'becomes', 'whence', 'nor', 'but', 'neither'}\n",
      "326\n"
     ]
    }
   ],
   "source": [
    "print(nlp.Defaults.stop_words)\n",
    "print(len(nlp.Defaults.stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
